{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "919SYOcod-qR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a 2 layer neural network\n",
    "\n",
    "Hi!, in this task we will implement a 2 layer fully connected neural network from scratch, using only numpy. For that, we will go through all the steps, explaning all the math behind to derivate all the equations using plain linear algebra and some basic calculus. In this way, we get full understanding of a neural network. Let's us do some recap first:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let be A and B two matrixes:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial AB}{\\partial A} &= B^T \\\\\n",
    "\\frac{\\partial AB}{\\partial B} &= A^T\n",
    "\\end{align*}\n",
    "$$\n",
    "- Sum of square errors:\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "- Gradient descend method, for a cost function $J$:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\theta$: Parameters (weights and biases)\n",
    "- $\\alpha$: Learning rate\n",
    "- $\\frac{\\partial J(\\theta)}{\\partial \\theta}$: Gradient of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo-TT13XiRiU"
   },
   "outputs": [],
   "source": [
    "def sserror(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Implement sum of square error\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    y_true -- vector with n elements\n",
    "    y_pred -- vector with n elements\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    sse - sum of square errors (scalar)\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    sse = #Your code here\n",
    "    #YOUR CODE ENDS HERE\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sserror(y_true, y_pred):\n",
    "    sse = 0.5 * np.sum(np.power(y_pred - y_true, 2))\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "Suppose you have a data of $m$ observations and $k$ features. So for a neural network the idea is to start with an input of $k$ neurons and an output of $n$ neurons.\n",
    "$$\n",
    "\\begin{align*}\n",
    "Z &= WX+b\\\\\n",
    "A &= g(Z)\n",
    "\\end{align*}\n",
    "$$\n",
    "where:\n",
    "- X is the input matrix with dimension $k \\times m$\n",
    "- W is the matrix of weights of dimension $n \\times k$\n",
    "- b is the vector of bias of dimension $n \\times 1$. If we are rigorous, this must be a matrix of dimension $(n,m)$\n",
    "- Z is the pre activated output, a matrix of dimension $n \\times m$\n",
    "- $g$ is the activation function (e.g., ReLU, sigmoid, tanh)  \n",
    "- A is the activated output, a matrix of dimension $n \\times m$\n",
    "\n",
    "So, What are these $\\mathcal{activation}$ $\\mathcal{functions}$ $g$? These are functions that add non linearity to our model. Otherwise this will be just linear combinations as a traditional linear regression. We will implement only 3 of them:\n",
    "\n",
    "Sigmoid:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "Tanh:\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "Relu:\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-NJlbwxwak4"
   },
   "outputs": [],
   "source": [
    "def actfunc(x, funct) :\n",
    "    \"\"\"\n",
    "    Implement activation functions\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    x -- input data \n",
    "    funct -- activation function, f(x)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    f -- funct activation function \n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    if funct == 'sigmoid' :\n",
    "        f = #Your code here\n",
    "    elif funct == 'tanh' :\n",
    "        f = #Your code here\n",
    "    elif funct == 'Relu' :\n",
    "        f =  #Your code here\n",
    "    else :\n",
    "        f = x\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actfunc(x, funct) :\n",
    "    \"\"\"\n",
    "    Implement activation functions\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    x -- input data \n",
    "    funct -- activation function, f(x)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    f -- funct activation function \n",
    "    \n",
    "    \"\"\"\n",
    "    if funct == 'sigmoid' :\n",
    "        f = 1/(1+np.exp(-x))\n",
    "    elif funct == 'tanh' :\n",
    "        f = np.tanh(x)\n",
    "    elif funct == 'Relu' :\n",
    "        f =  np.maximum(0,x)\n",
    "    else :\n",
    "        f = x\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestActivationFunction(unittest.TestCase):\n",
    "\n",
    "    def test_sigmoid(self):\n",
    "        x = np.array([0])\n",
    "        expected = 1 / (1 + np.exp(0))  # = 0.5\n",
    "        result = actfunc(x, 'sigmoid')\n",
    "        self.assertAlmostEqual(result[0], expected, msg=f\"Expected sigmoid(0) to be {expected}, but got {result[0]}\")\n",
    "\n",
    "    def test_tanh(self):\n",
    "        x = np.array([0])\n",
    "        expected = np.tanh(0)  # = 0.0\n",
    "        result = actfunc(x, 'tanh')\n",
    "        self.assertAlmostEqual(result[0], expected, msg=f\"Expected tanh(0) to be {expected}, but got {result[0]}\")\n",
    "\n",
    "    def test_relu_positive(self):\n",
    "        x = np.array([5])\n",
    "        expected = np.maximum(0, 5)  # = 5\n",
    "        result = actfunc(x, 'Relu')\n",
    "        self.assertEqual(result[0], expected, msg=f\"Expected Relu(5) to be {expected}, but got {result[0]}\")\n",
    "\n",
    "    def test_relu_negative(self):\n",
    "        x = np.array([-3])\n",
    "        expected = 0\n",
    "        result = actfunc(x, 'Relu')\n",
    "        self.assertEqual(result[0], expected, msg=f\"Expected Relu(-3) to be {expected}, but got {result[0]}\")\n",
    "\n",
    "    def test_default(self):\n",
    "        x = np.array([7])\n",
    "        # No known function provided â†’ return input\n",
    "        result = actfunc(x, 'unknown')\n",
    "        self.assertTrue(np.array_equal(result, x), msg=f\"Expected input array to be returned, but got {result} instead\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In general, for a layer $l$, the forward propagation equations are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Z^{[l]} &= W^{[l]} A^{[l-1]} + b^{[l]} \\\\\n",
    "A^{[l]} &= g^{[l]}(Z^{[l]})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ A^{[l-1]} $: Activated output from the previous layer with dimension $n^{[l-1]} \\times m$ \n",
    "- $ W^{[l]} $: Weight matrix for layer with dimension $ n^{[l]} \\times n^{[l-1]}$\n",
    "- $ b^{[l]} $: Bias vector for layer with dimension $n^{[l]} \\times 1$\n",
    "- $ Z^{[l]}$: Pre activated output with dimension $n^{[l]} \\times m $   \n",
    "- $ g^{[l]} $: Activation function (e.g., ReLU, sigmoid, tanh) of layer $l$\n",
    "- $A^{[l]} $: Activated output with dimension $n^{[l]} \\times m$  \n",
    "- $ m $: Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Twd811Bj5IXF"
   },
   "outputs": [],
   "source": [
    "def forwardprop(X,W1,W2,b1,b2, funct1, funct2) :\n",
    "    \"\"\"\n",
    "    Implement forward propagation with 2 layers\n",
    "    \n",
    "    Arguments:\n",
    "    X: input data of shape (n_inputlayer, m)\n",
    "    W1: weights of 1st layer, shape (n_hiddenlayer, n_inputlayer)\n",
    "    b1: bias of 1st layer, shape (n_hiddenlayer, 1)\n",
    "    W2: weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    b2: bias of 2nd layer, shape (n_outputlayer, 1)\n",
    "\n",
    "    Returns:\n",
    "    A2: activated output of 2nd layer, shape (n_outputlayer,m)\n",
    "    Z2: preactivated output of 2nd layer, shape (n_outputlayer,m)\n",
    "    A1: activated output of 1st layer, shape (n_hiddenlayer,m)\n",
    "    Z1: preactivated output of 1st layer, shaoe (n_hiddenlayer,m)\n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    Z1 = #your code here\n",
    "    A1 = #your code here\n",
    "    Z2 = #your code here\n",
    "    A2 = #your code here\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return A2, Z2, A1, Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop(X,W1,W2,b1,b2, funct1, funct2) :\n",
    "    \"\"\"\n",
    "    Implement forward propagation with 2 layers\n",
    "    \n",
    "    Arguments:\n",
    "    X: input data of shape (n_inputlayer, m)\n",
    "    W1: weights of 1st layer, shape (n_hiddenlayer, n_inputlayer)\n",
    "    b1: bias of 1st layer, shape (n_hiddenlayer, 1)\n",
    "    W2: weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    b2: bias of 2nd layer, shape (n_outputlayer, 1)\n",
    "\n",
    "    Returns:\n",
    "    A2: activated output of 2nd layer, shape (n_outputlayer,m)\n",
    "    Z2: preactivated output of 2nd layer, shape (n_outputlayer,m)\n",
    "    A1: activated output of 1st layer, shape (n_hiddenlayer,m)\n",
    "    Z1: preactivated output of 1st layer, shaoe (n_hiddenlayer,m)\n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    Z1 = W1.dot(X)+b1\n",
    "    A1 = actfunc(Z1, funct1)\n",
    "    Z2 = W2.dot(A1)+b2\n",
    "    A2 = actfunc(Z2, funct2)\n",
    "    return A2, Z2, A1, Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestForwardProp(unittest.TestCase):\n",
    "\n",
    "    def test_forwardprop_shapes(self):\n",
    "        # Setup dummy input\n",
    "        X = np.array([[0.5], [0.1]])  # shape: (2, 1)\n",
    "        W1 = np.array([[0.2, 0.4], [0.3, 0.1]])  # shape: (2, 2)\n",
    "        b1 = np.array([[0.1], [0.2]])           # shape: (2, 1)\n",
    "        W2 = np.array([[0.7, 0.5]])             # shape: (1, 2)\n",
    "        b2 = np.array([[0.3]])                  # shape: (1, 1)\n",
    "\n",
    "        # Call forwardprop\n",
    "        A2, Z2, A1, Z1 = forwardprop(X, W1, W2, b1, b2, 'tanh', 'sigmoid')\n",
    "\n",
    "        # Check output shapes\n",
    "        self.assertEqual(A2.shape, (1, 1), msg=f\"Expected A2 shape (1,1), got {A2.shape}\")\n",
    "        self.assertEqual(Z2.shape, (1, 1), msg=f\"Expected Z2 shape (1,1), got {Z2.shape}\")\n",
    "        self.assertEqual(A1.shape, (2, 1), msg=f\"Expected A1 shape (2,1), got {A1.shape}\")\n",
    "        self.assertEqual(Z1.shape, (2, 1), msg=f\"Expected Z1 shape (2,1), got {Z1.shape}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward propagation outputs in the last layer the predicted variable, given known parameters $W$ and $b$. So for different values of these paremeters, we get different values of the predicted variable. How can we get the right $W$ and $b$ such that the predicted variable gets really closed to the ground truth label? This is where the backward propagation takes place\n",
    "## Backward propagation\n",
    "It is a procedure where we start from the loss function and we go all the way back to the begining, updating the weight and bias matrixes in each layer:\n",
    "\n",
    "Applying chain rule:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}} &= \\frac{\\partial J}{\\partial A^{[l]}}\\cdot \\frac{\\partial g^{[l]}(Z^{[l]})}{\\partial Z^{[l]}} \\\\\n",
    "\\frac{\\partial J}{\\partial W^{[l]}} &= \\frac{\\partial J}{\\partial Z^{[l]}}\\cdot \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}\n",
    "= \\frac{\\partial J}{\\partial Z^{[l]}}\\cdot (A^{[l]})^T \\\\\n",
    "\\frac{\\partial J}{\\partial b^{[l]}} &= \\frac{\\partial J}{\\partial Z^{[l]}}\\cdot \\frac{\\partial Z^{[l]}}{\\partial b^{[l]}}\n",
    "= \\frac{\\partial J}{\\partial Z^{[l]}}\\cdot \\mathbf{1} \\\\\n",
    "\\frac{\\partial J}{\\partial A^{[l-1]}} &= \\frac{\\partial J}{\\partial Z^{[l]}}\\cdot \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}}= (W^{[l]})^T \\cdot \\frac{\\partial J}{\\partial Z^{[l]}}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, we need the derivative of the activation function $g$. The derivative of the functions we have considered for this task are:\n",
    "\n",
    "Derivative of sigmoid:\n",
    "$$\n",
    "\\frac{d}{dx} \\sigma(x) = \\sigma(x) \\left( 1 - \\sigma(x) \\right)\n",
    "$$\n",
    "Derivative of tanh:\n",
    "$$\n",
    "\\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x)\n",
    "$$\n",
    "Derivative of ReLU:\n",
    "$$\n",
    "\\frac{d}{dx} \\text{ReLU}(x) =\n",
    "\\begin{cases} \n",
    "0 & \\text{if } x < 0 \\\\\n",
    "1 & \\text{if } x \\geq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHiCwt4T1f6L"
   },
   "outputs": [],
   "source": [
    "def derivactfunc(x, funct) :\n",
    "    \"\"\"\n",
    "    Implement derivative activation functions\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    x -- input data \n",
    "    funct -- activation function, f(x)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    df -- derivative of funct activation function, f'(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    if funct == 'sigmoid' :\n",
    "        f = #your code here\n",
    "        df = #your code here\n",
    "    elif funct == 'tanh' :\n",
    "        f = #your code here\n",
    "        df = #your code here\n",
    "    elif funct == 'Relu' :\n",
    "        f =  #your code here\n",
    "        df = #your code here\n",
    "    else :\n",
    "        df = 1\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivactfunc(x, funct) :\n",
    "    \"\"\"\n",
    "    Implement derivative activation functions\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    x -- input data \n",
    "    funct -- activation function, f(x)\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    df -- derivative of funct activation function, f'(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    if funct == 'sigmoid' :\n",
    "        f = 1/(1+np.exp(-x))\n",
    "        df = f*(1-f)\n",
    "    elif funct == 'tanh' :\n",
    "        f = np.tanh(x)\n",
    "        df = 1-f**2\n",
    "    elif funct == 'Relu' :\n",
    "        f =  np.maximum(0,x)\n",
    "        df = f > 0\n",
    "    else :\n",
    "        df = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDerivActFunc(unittest.TestCase):\n",
    "\n",
    "    def test_sigmoid_derivative(self):\n",
    "        x = np.array([0.0])\n",
    "        expected = 0.25  # sigmoid(0) = 0.5, derivative = 0.5 * (1 - 0.5)\n",
    "        result = derivactfunc(x, 'sigmoid')\n",
    "        self.assertAlmostEqual(result[0], expected, msg=f\"Expected sigmoid' at 0 to be {expected}, got {result[0]}\")\n",
    "\n",
    "    def test_tanh_derivative(self):\n",
    "        x = np.array([0.0])\n",
    "        expected = 1.0  # tanh(0) = 0, derivative = 1 - 0^2 = 1\n",
    "        result = derivactfunc(x, 'tanh')\n",
    "        self.assertAlmostEqual(result[0], expected, msg=f\"Expected tanh' at 0 to be {expected}, got {result[0]}\")\n",
    "\n",
    "    def test_relu_derivative(self):\n",
    "        x = np.array([-1.0, 0.0, 2.0])\n",
    "        expected = np.array([0.0, 0.0, 1.0])\n",
    "        result = derivactfunc(x, 'Relu')\n",
    "        np.testing.assert_array_equal(result, expected, err_msg=f\"Expected ReLU' at {x} to be {expected}, got {result}\")\n",
    "\n",
    "    def test_unknown_function(self):\n",
    "        x = np.array([42.0])\n",
    "        expected = np.array([1.0])\n",
    "        result = derivactfunc(x, 'unknown')\n",
    "        np.testing.assert_array_equal(result, expected, err_msg=\"Expected derivative of unknown to return ones.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous equations are often written using differential notations, e.g, $db^{[l]} = \\frac{\\partial J}{\\partial b^{[l]}}$ , so the Backward equations are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    " dZ^{[l]} &= dA^{[l]} * g'^{[l]}(Z^{[l]}) \\\\\n",
    " dW^{[l]} &= \\frac{1}{m}dZ^{[l]}\\cdot (A^{[l]})^T \\\\\n",
    " db^{[l]} &= \\frac{1}{m}dZ^{[l]}\\cdot \\mathbf{1} \\\\\n",
    " dA^{[l-1]} &= (W^{[l]})^T \\cdot dZ^{[l]}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $*$ is the element-wise multiplication\n",
    "- $\\cdot$ is the matrix multiplication \n",
    "\n",
    "Notice that we have included now the term $\\frac{1}{m} $ where $m$ is the batch size. This is to average the parameters to make it independent of the batch size.\n",
    "\n",
    "In this task we consider the sum square error:\n",
    "$$\n",
    "J( A,Y) = \\frac{( A-Y)^2}{2}\n",
    "$$\n",
    "thus\n",
    "$$\n",
    "dA =  A-Y\n",
    "$$\n",
    "where $A$ is the predicted output in the last layer and $Y$ is the ground truth label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM4_hcsG6ZUD"
   },
   "outputs": [],
   "source": [
    "def backwardprop(X, y_true, A2, Z2, A1, Z1, W2, funct1, funct2):\n",
    "    \"\"\"\n",
    "    Implement backwad propagation equations\n",
    "    \n",
    "    Arguments:\n",
    "    X: Input data, shape (n_inputlayer, m)\n",
    "    y_true: Ground truth label, shape (n_outputlayer, m) \n",
    "    W2: weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    A2: activated output of 2nd layer, shape (n_outputlayer,m)\n",
    "    Z2: preactivated output of 2nd layer, shape (n_outputlayer,m)\n",
    "    A1: activated output of 1st layer, shape (n_hiddenlayer,m)\n",
    "    Z1: preactivated output of 1st layer, shaoe (n_hiddenlayer,m)\n",
    "    funct1: activation function of 1st layer\n",
    "    funct2: activation function of 2nd layer\n",
    "        \n",
    "    Returns:\n",
    "    dW1: Gradient of loss w.r.t. W1, shape (n_outputlayer,m)\n",
    "    db1: Gradient of loss w.r.t. b1, shape (n_hiddenlayer,1)\n",
    "    dW2: Gradient of loss w.r.t. W2, shape (n_outputlayer,n_hiddenlayer)\n",
    "    db2: Gradient of loss w.r.t. b2, shape (n_outputlayer,1)\n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    m = # your code here, batch size\n",
    "    dA2 = # your code here, shape (n_outputlayer,m) -- consider A-Y \n",
    "    dZ2 = # your code here, shape (n_outputlayer,m) -- element wise multiplication of dA2 and funct1'(Z2)\n",
    "    dW2 = # your code here, shape (n_outputlayer,n_hiddenlayer)\n",
    "    db2 = # your code here, shape (n_outputlayer,1)\n",
    "    ###################\n",
    "    dA1 = # your code here, shape (n_hiddenlayer,m)\n",
    "    dZ1 = # your code here, shape (n_hiddenlayer,m) -- element wise multiplication of dA1 and funct2'(Z1)\n",
    "    dW1 = # your code here, shape (n_hiddenlayer,n_inputlayer)\n",
    "    db1 = # your code here, shape (n_hiddenlayer,1)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return db2, dW2, db1, dW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardprop(X, y_true, A2, Z2, A1, Z1, W2, funct1, funct2):\n",
    "    \"\"\"\n",
    "    Implement backwad propagation equations\n",
    "    \n",
    "    Arguments:\n",
    "    X: Input data, shape (n_inputlayer, m)\n",
    "    y_true: Ground truth label, shape (n_outputlayer, m) \n",
    "    W2: weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    A2: activated output of 2nd layer, shape (n_outputlayer,m)\n",
    "    Z2: preactivated output of 2nd layer, shape (n_outputlayer,m)\n",
    "    A1: activated output of 1st layer, shape (n_hiddenlayer,m)\n",
    "    Z1: preactivated output of 1st layer, shaoe (n_hiddenlayer,m)\n",
    "    funct1: activation function of 1st layer\n",
    "    funct2: activation function of 2nd layer\n",
    "        \n",
    "    Returns:\n",
    "    dW1: Gradient of loss w.r.t. W1, shape (n_hiddenlayer,m)\n",
    "    db1: Gradient of loss w.r.t. b1, shape (n_hiddenlayer,1)\n",
    "    dW2: Gradient of loss w.r.t. W2, shape (n_outputlayer,n_hiddenlayer)\n",
    "    db2: Gradient of loss w.r.t. b2, shape (n_outputlayer,1)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    dA2 = (A2-y_true)\n",
    "    dZ2 = dA2*derivactfunc(Z2, funct2)\n",
    "    dW2 = dZ2.dot(A1.T)/m\n",
    "    db2 = dZ2.dot(np.ones((dZ2.shape[1], 1)))/m\n",
    "    ###################\"\n",
    "    dA1 = (W2.T).dot(dZ2)\n",
    "    dZ1 = dA1*derivactfunc(Z1, funct1)\n",
    "    dW1 = dZ1.dot(X.T)/m\n",
    "    db1 = dZ1.dot(np.ones((dZ1.shape[1], 1)))/m\n",
    "    return db2, dW2, db1, dW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBackwardProp(unittest.TestCase):\n",
    "\n",
    "    def test_backwardprop_shapes(self):\n",
    "        # Setup dummy data\n",
    "        X = np.array([[0.5, 0.2], [0.1, 0.3]])  # shape: (2, 2)\n",
    "        y_true = np.array([[0.7, 0.8]])          # shape: (1, 2)\n",
    "        A2 = np.array([[0.6, 0.7]])              # shape: (1, 2)\n",
    "        Z2 = np.array([[0.1, 0.2]])              # shape: (1, 2)\n",
    "        A1 = np.array([[0.5, 0.6], [0.4, 0.5]])  # shape: (2, 2)\n",
    "        Z1 = np.array([[0.3, 0.4], [0.2, 0.3]])  # shape: (2, 2)\n",
    "        W2 = np.array([[0.1, 0.2]])              # shape: (1, 2)\n",
    "\n",
    "        # Call backwardprop\n",
    "        db2, dW2, db1, dW1 = backwardprop(X, y_true, A2, Z2, A1, Z1, W2, 'sigmoid', 'sigmoid')\n",
    "\n",
    "        # Check shapes of the gradients\n",
    "        self.assertEqual(dW1.shape, (2, 2), msg=f\"Expected dW1 shape (2, 2), got {dW1.shape}\")\n",
    "        self.assertEqual(db1.shape, (2, 1), msg=f\"Expected db1 shape (2, 1), got {db1.shape}\")\n",
    "        self.assertEqual(dW2.shape, (1, 2), msg=f\"Expected dW2 shape (1, 2), got {dW2.shape}\")\n",
    "        self.assertEqual(db2.shape, (1, 1), msg=f\"Expected db2 shape (1, 1), got {db2.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Once we have compute all the derivatives, we plug them into the gradient descent method to update the weights and bias:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "W^{[l]} & = W^{[l]} - \\alpha \\,dW^{[l]}\\\\\n",
    "b^{[l]} & = b^{[l]} - \\alpha \\, db^{[l]}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\alpha$ is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvK9tbwi6R-r"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(b1, b2, W1, W2,db2, dW2, db1, dW1, lr) :\n",
    "    \"\"\"\n",
    "    Implement gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    b1: bias of 1st layer, shape (n_hiddenlayer, 1)\n",
    "    b2: bias of 2nd layer, shape (n_outputlayer, 1)\n",
    "    W1: weights of 1st layer, shape (n_hiddenlayer, n_inputlayer)\n",
    "    W2: weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    dW1: Gradient of loss w.r.t. W1, shape (n_hiddenlayer,n_inputlayer)\n",
    "    db1: Gradient of loss w.r.t. b1, shape (n_hiddenlayer,1)\n",
    "    dW2: Gradient of loss w.r.t. W2, shape (n_outputlayer,n_hiddenlayer)\n",
    "    db2: Gradient of loss w.r.t. b2, shape (n_outputlayer,1)\n",
    "    lr: learning rate\n",
    "        \n",
    "    Returns:\n",
    "    b1: Updated bias of 1st layer, shape (n_hiddenlayer, 1)\n",
    "    b2: Updated bias of 2nd layer, shape (n_outputlayer, 1)\n",
    "    W1: Updated weights of 1st layer, shape (n_hiddenlayer, n_inputlayer)\n",
    "    W2: Updated weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    b1 = #your code here\n",
    "    b2 = #your code here\n",
    "    W1 = #your code here\n",
    "    W2 = #your code here\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return b1, b2, W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(b1, b2, W1, W2,db2, dW2, db1, dW1, learning_rate) :\n",
    "    \"\"\"\n",
    "    Implement gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    b1: bias of 1st layer, shape (n_hiddenlayer, 1)\n",
    "    b2: bias of 2nd layer, shape (n_outputlayer, 1)\n",
    "    W1: weights of 1st layer, shape (n_hiddenlayer, n_inputlayer)\n",
    "    W2: weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    dW1: Gradient of loss w.r.t. W1, shape (n_hiddenlayer,n_inputlayer)\n",
    "    db1: Gradient of loss w.r.t. b1, shape (n_hiddenlayer,1)\n",
    "    dW2: Gradient of loss w.r.t. W2, shape (n_outputlayer,n_hiddenlayer)\n",
    "    db2: Gradient of loss w.r.t. b2, shape (n_outputlayer,1)\n",
    "    lr: learning rate\n",
    "        \n",
    "    Returns:\n",
    "    b1: Updated bias of 1st layer, shape (n_hiddenlayer, 1)\n",
    "    b2: Updated bias of 2nd layer, shape (n_outputlayer, 1)\n",
    "    W1: Updated weights of 1st layer, shape (n_hiddenlayer, n_inputlayer)\n",
    "    W2: Updated weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    \"\"\"\n",
    "    b1 = b1-learning_rate*db1\n",
    "    b2 = b2-learning_rate*db2\n",
    "    W1 = W1-learning_rate*dW1\n",
    "    W2 = W2-learning_rate*dW2\n",
    "    return b1, b2, W1, W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_gradient_descent_shapes(self):\n",
    "        # Setup dummy data\n",
    "        b1 = np.array([[0.1], [0.2]])  # shape: (2, 1)\n",
    "        b2 = np.array([[0.3]])         # shape: (1, 1)\n",
    "        W1 = np.array([[0.4, 0.5], [0.6, 0.7]])  # shape: (2, 2)\n",
    "        W2 = np.array([[0.8, 0.9]])  # shape: (1, 2)\n",
    "        db2 = np.array([[0.1]])  # shape: (1, 1)\n",
    "        dW2 = np.array([[0.2, 0.3]])  # shape: (1, 2)\n",
    "        db1 = np.array([[0.4], [0.5]])  # shape: (2, 1)\n",
    "        dW1 = np.array([[0.6, 0.7], [0.8, 0.9]])  # shape: (2, 2)\n",
    "        learning_rate = 0.01\n",
    "\n",
    "        # Call gradient_descent\n",
    "        b1_updated, b2_updated, W1_updated, W2_updated = gradient_descent(\n",
    "            b1, b2, W1, W2, db2, dW2, db1, dW1, learning_rate\n",
    "        )\n",
    "\n",
    "        # Check shapes of the updated parameters\n",
    "        self.assertEqual(b1_updated.shape, b1.shape, msg=f\"Expected b1 shape {b1.shape}, got {b1_updated.shape}\")\n",
    "        self.assertEqual(b2_updated.shape, b2.shape, msg=f\"Expected b2 shape {b2.shape}, got {b2_updated.shape}\")\n",
    "        self.assertEqual(W1_updated.shape, W1.shape, msg=f\"Expected W1 shape {W1.shape}, got {W1_updated.shape}\")\n",
    "        self.assertEqual(W2_updated.shape, W2.shape, msg=f\"Expected W2 shape {W2.shape}, got {W2_updated.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning process\n",
    "This is an iterative process where parameters are repeatedly updated, aiming to make the error approach zero:\n",
    "\n",
    " step 1: Forward propagation given parameters \\\n",
    " step 2: Calculate error \\\n",
    " step 3: Backward propagation \\\n",
    " step 4: Update parameters with gradient descent \\\n",
    " step 5: Repeat steps 1 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLcFYg41i9Gx"
   },
   "outputs": [],
   "source": [
    "def trainproc(x_train, y_train, n_inputlayer, n_hiddenlayer, n_outputlayer, iteration, lr, actfunct1,actfunct2 ) :\n",
    "    \"\"\"\n",
    "    Implement training algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    x_train:train data, shape (m, n_inputlayer)\n",
    "    y_train: ground truth label, shape (n_outputlayer, m)\n",
    "    n_inputlayer: number of neurons in the input layer\n",
    "    n_hiddenlayer: number of neurons in the hidden layer\n",
    "    n_outputlayer: number of neurons in the output layer\n",
    "    iteration: number of iterations for training\n",
    "    lr: learning rate\n",
    "    actfunct1: activation function in the 1st layer\n",
    "    actfunct2: activation function in the 2nd layer\n",
    "        \n",
    "    Returns:\n",
    "    b1: Updated bias of 1st layer, shape (n_hiddenlayer, 1)\n",
    "    b2: Updated bias of 2nd layer, shape (n_outputlayer, 1)\n",
    "    W1: Updated weights of 1st layer, shape (n_hiddenlayer, n_inputlayer)\n",
    "    W2: Updated weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # Initialize W1,b1, W2 and b2 with random values you can use np.random.rand(n_x,n_y)\n",
    "    np.random.seed(0) \n",
    "    W1 = #your code here - initialize W1\n",
    "    b1 = #your code here - initialize b1\n",
    "    W2 = #your code here - initialize W2\n",
    "    b2 = #your code here - initialize b2\n",
    "    error = #your code here - initialize error with cero\n",
    "    for i in range(iteration) :\n",
    "        #call the forward propagation function in this line\n",
    "        if n_outputlayer == 1 :  \n",
    "            error =  # your code here, call the SSE function. This type of error only works when n_outputlayer = 1\n",
    "        else\n",
    "            error = # your code here, otherwhise cero.\n",
    "        #call the backward propagation function in this line\n",
    "        #call the gradient descendent function in this line\n",
    "        #print the iteration and the error \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return b1, b2, W1, W2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainproc(x_train, y_train, n_inputlayer, n_hiddenlayer, n_outputlayer, iteration, lr, actfunct1,actfunct2 ) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement training algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    x_train:train data, shape (m, n_inputlayer)\n",
    "    y_train: ground truth label, shape (n_outputlayer, m)\n",
    "    n_inputlayer: number of neurons in the input layer\n",
    "    n_hiddenlayer: number of neurons in the hidden layer\n",
    "    n_outputlayer: number of neurons in the output layer\n",
    "    iteration: number of iterations for training\n",
    "    lr: learning rate\n",
    "    actfunct1: activation function in the 1st layer\n",
    "    actfunct2: activation function in the 2nd layer\n",
    "        \n",
    "    Returns:\n",
    "    b1: Updated bias of 1st layer, shape (n_hiddenlayer, 1)\n",
    "    b2: Updated bias of 2nd layer, shape (n_outputlayer, 1)\n",
    "    W1: Updated weights of 1st layer, shape (n_hiddenlayer, n_inputlayer)\n",
    "    W2: Updated weights of 2nd layer, shape (n_outputlayer, n_hiddenlayer)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    W1 = np.random.rand(n_hiddenlayer, n_inputlayer)\n",
    "    b1 = np.random.rand(n_hiddenlayer,1)\n",
    "    W2 = np.random.rand(n_outputlayer, n_hiddenlayer)\n",
    "    b2 = np.random.rand(n_outputlayer,1)\n",
    "    error = 0\n",
    "    for i in range(iteration) :\n",
    "        A2, Z2, A1, Z1 = forwardprop(x_train,W1,W2,b1,b2, actfunct1, actfunct2)\n",
    "        if n_outputlayer == 1:\n",
    "            error = sserror(y_train,A2)\n",
    "        else :\n",
    "            error = 0\n",
    "        db2, dW2, db1, dW1 = backwardprop(x_train, y_train, A2, Z2, A1, Z1, W2, actfunct1, actfunct2)\n",
    "        b1, b2, W1, W2 = gradient_descent(b1, b2, W1, W2, db2, dW2, db1, dW1, lr)\n",
    "        print('iteration :', i , 'error:', error)\n",
    "    return b1, b2, W1, W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example test - XOR table\n",
    "\n",
    "Now it is time to validate your implementation with experimental data. The XOR table it is not linear separable so it is\n",
    "a good example to test your neural network. For this, consider the next:\n",
    "- x_train is the XOR table 4 rows and 2 columns\n",
    "- y_train is XOR output, 4 rows and 1 column\n",
    "- n_inputlayer = 2\n",
    "- n_hiddenlayer = 4\n",
    "- n_outputlayer = 1\n",
    "- iterations = 500\n",
    "- learning rate = 0.1\n",
    "- actfunct1 = 'tanh'\n",
    "- actfunct2 = 'Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg5_H1yUHE8X"
   },
   "outputs": [],
   "source": [
    "x_train = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_train = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Do you need to reshape/Transpose your x_train, y_train?\n",
    "b1, b2, W1, W2 = #your code here, call trainproc with the previous settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we get the prediction from forward propagation, Use W1,W2,b1 and b2 from the previous cell and compare it with y_train\n",
    "np.round(forwardprop(,,,,,,)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
